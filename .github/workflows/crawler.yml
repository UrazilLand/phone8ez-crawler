name: SmartChoice Crawler

on:
  workflow_dispatch:
  schedule:
    - cron: '0 15 * * *'  # ë§¤ì¼ ìì • ì‹¤í–‰ (UTC ê¸°ì¤€)

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
    - name: ì €ì¥ì†Œ ì²´í¬ì•„ì›ƒ
      uses: actions/checkout@v3
      with:
        persist-credentials: false

    - name: Python ì„¤ì¹˜
      uses: actions/setup-python@v4
      with:
        python-version: '3.8'

    - name: ì˜ì¡´ì„± ì„¤ì¹˜
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Chrome ì„¤ì¹˜
      run: |
        sudo apt-get update
        sudo apt-get install -y chromium-browser

    - name: ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
      run: |
        mkdir -p data

    - name: ì›¹ í¬ë¡¤ëŸ¬ ì‹¤í–‰ (ë°°ì¹˜ ëª¨ë“œ)
      run: |
        python -c "
        from smartchoice_crawler import SmartChoiceCrawler
        import time
        
        # í¬ë¡¤ë§í•  ëª¨ë¸ ëª©ë¡
        models = [
            ('ì‚¼ì„±ì „ì', 'ê°¤ëŸ­ì‹œ S24'),
            ('ì‚¼ì„±ì „ì', 'ê°¤ëŸ­ì‹œ A55'),
            ('ì• í”Œ', 'iPhone 15'),
            ('ì• í”Œ', 'iPhone 15 Pro')
        ]
        
        results = []
        
        with SmartChoiceCrawler(headless=True) as crawler:
            crawler.navigate_to_page()
            
            for manufacturer, model in models:
                try:
                    print(f'í¬ë¡¤ë§ ì¤‘: {manufacturer} - {model}')
                    crawler.select_manufacturer(manufacturer)
                    
                    if crawler.select_model(model):
                        if crawler.search_support_info():
                            result = crawler.crawl_model_with_summary(manufacturer, model)
                            if result:
                                results.append(result)
                                print(f'âœ… ì„±ê³µ: {model}')
                            else:
                                print(f'âŒ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {model}')
                        else:
                            print(f'âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {model}')
                    else:
                        print(f'âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model}')
                    
                    crawler.reset_search()
                    time.sleep(3)
                    
                except Exception as e:
                    print(f'âŒ ì˜¤ë¥˜ ë°œìƒ: {manufacturer} - {model}: {e}')
                    continue
        
        # ê²°ê³¼ ì €ì¥
        if results:
            crawler.save_summary_data(results, 'data/batch_crawling_results.json')
            print(f'âœ… {len(results)}ê°œ ëª¨ë¸ í¬ë¡¤ë§ ì™„ë£Œ')
        else:
            print('âŒ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤')
        "

    - name: 30ì¼ ì§€ë‚œ íŒŒì¼ ì‚­ì œ
      run: |
        find data/ -name '*.json' -type f -mtime +30 -print -delete
        
    - name: Git ì„¤ì • ë° ê²°ê³¼ í‘¸ì‹œ
      env:
        TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        git config --global user.name "UrailLand"
        git config --global user.email "urailland@users.noreply.github.com"
        
        # ìƒˆë¡œ ìƒì„±ëœ ëª¨ë“  íŒŒì¼ì„ í¬í•¨í•˜ì—¬ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€
        git add data/ || echo "ì¶”ê°€í•  íŒŒì¼ ì—†ìŒ"

        git commit -m "ğŸ“¦ ìë™ í¬ë¡¤ë§ ê²°ê³¼ ë°˜ì˜ - $(date)" || echo "ë³€ê²½ ì—†ìŒ"
        git pull --rebase origin main || echo "ì¶©ëŒ ì—†ìŒ"
        git push https://x-access-token:${TOKEN}@github.com/${{ github.repository }}.git
