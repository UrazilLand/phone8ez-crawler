name: SmartChoice Crawler

on:
  workflow_dispatch:
  schedule:
    - cron: '0 15 * * *'  # 매일 자정 실행 (UTC 기준)

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
    - name: 저장소 체크아웃
      uses: actions/checkout@v3
      with:
        persist-credentials: false

    - name: Python 설치
      uses: actions/setup-python@v4
      with:
        python-version: '3.8'

    - name: 의존성 설치
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Chrome 설치
      run: |
        sudo apt-get update
        sudo apt-get install -y chromium-browser

    - name: 데이터 디렉토리 생성
      run: |
        mkdir -p data

    - name: 웹 크롤러 실행 (배치 모드)
      run: |
        python -c "
        from smartchoice_crawler import SmartChoiceCrawler
        import time
        
        # 크롤링할 모델 목록
        models = [
            ('삼성전자', '갤럭시 S24'),
            ('삼성전자', '갤럭시 A55'),
            ('애플', 'iPhone 15'),
            ('애플', 'iPhone 15 Pro')
        ]
        
        results = []
        
        with SmartChoiceCrawler(headless=True) as crawler:
            crawler.navigate_to_page()
            
            for manufacturer, model in models:
                try:
                    print(f'크롤링 중: {manufacturer} - {model}')
                    crawler.select_manufacturer(manufacturer)
                    
                    if crawler.select_model(model):
                        if crawler.search_support_info():
                            result = crawler.crawl_model_with_summary(manufacturer, model)
                            if result:
                                results.append(result)
                                print(f'✅ 성공: {model}')
                            else:
                                print(f'❌ 데이터 추출 실패: {model}')
                        else:
                            print(f'❌ 검색 결과 없음: {model}')
                    else:
                        print(f'❌ 모델을 찾을 수 없음: {model}')
                    
                    crawler.reset_search()
                    time.sleep(3)
                    
                except Exception as e:
                    print(f'❌ 오류 발생: {manufacturer} - {model}: {e}')
                    continue
        
        # 결과 저장
        if results:
            crawler.save_summary_data(results, 'data/batch_crawling_results.json')
            print(f'✅ {len(results)}개 모델 크롤링 완료')
        else:
            print('❌ 크롤링된 데이터가 없습니다')
        "

    - name: 30일 지난 파일 삭제
      run: |
        find data/ -name '*.json' -type f -mtime +30 -print -delete
        
    - name: Git 설정 및 결과 푸시
      env:
        TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        git config --global user.name "UrailLand"
        git config --global user.email "urailland@users.noreply.github.com"
        
        # 새로 생성된 모든 파일을 포함하여 명시적으로 추가
        git add data/ || echo "추가할 파일 없음"

        git commit -m "📦 자동 크롤링 결과 반영 - $(date)" || echo "변경 없음"
        git pull --rebase origin main || echo "충돌 없음"
        git push https://x-access-token:${TOKEN}@github.com/${{ github.repository }}.git
